{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"2CKKAGieG-Wx"},"outputs":[],"source":["#Imports\n","%matplotlib inline\n","import pandas as pd\n","import numpy as np\n","import math\n","import matplotlib.pylab as plt\n","import os\n","import tensorflow as tf\n","from tensorflow import keras\n","from keras.layers import Dropout\n","import pydot\n","\n","import warnings\n","warnings.filterwarnings('ignore')\n","os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' #INFO and WARNING messages are not printed\n","import time"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9M_bUB5BHT0B"},"outputs":[],"source":["\n","\n","# Load Transactions Data\n","interactions=pd.read_csv('HM_interactions.csv')\n","interactions"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tEsgyDbmHeX0"},"outputs":[],"source":["#Test set creation\n","\n","# convert timestamp to datetime\n","interactions['timestamp'] = pd.to_datetime(interactions['timestamp'], unit='s')\n","# sort by date, group by customer_id and choose the most recent\n","interactions_sorted = interactions.sort_values(['timestamp'],ascending=True).groupby('customer_id').tail(1)\n","#  we create the test_set with the most recent purchase\n","test_set = interactions_sorted[['customer_id', 'article_id']].copy()\n","# add \"interactions\" column\n","test_set['interaction']='1'\n","test_set"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fw1I4oq9Jgeh"},"outputs":[],"source":["# Train set creation\n","\n","# rows to be dropped are selectect (test_set) \n","rows_to_drop = list(test_set.index.values)\n","# rows are dropped\n","pre_train_set = interactions.drop(rows_to_drop)\n","# order interactions by clients\n","pre_train_set = pre_train_set.sort_values(by=['customer_id'])\n","#we select \"customer_id\" and \"article_id\" columns and add interactions column\n","pre_train_set = pre_train_set[['customer_id', 'article_id']].copy()\n","# add interaction column\n","pre_train_set['interaction'] = 1\n","pre_train_set"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3XINuX2Bwpcx"},"outputs":[],"source":["# Negative sampling\n","\n","#transform to numpy for sampling\n","trans_matrix=pd.DataFrame(pre_train_set).to_numpy()\n","\n","# Client's list\n","customer_list=np.array(np.unique(trans_matrix[:,0]))\n","customer_list"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-yPOukfOxNmx"},"outputs":[],"source":["# negative sampling loop\n","negative_sampling_all=np.empty((0, 3), int)\n","for i in range(np.size(customer_list)):\n","  # save all articles bought by the i-th client\n","  find=trans_matrix[np.where(trans_matrix[:,0] == customer_list[i])][:,1]\n","  # we save 4 random articles not bought by the i-th customer\n","  notfind=np.random.choice(trans_matrix[trans_matrix[:,0]!=i][:,1],4*np.size(find))\n","  create_notfind=np.column_stack((np.transpose(np.full(np.size(trans_matrix[trans_matrix[:,0]==customer_list[i]][:,0])*4,customer_list[i])), np.transpose(notfind),np.zeros(np.size(trans_matrix[trans_matrix[:,0]==customer_list[i]][:,0])*4)))\n","  # train_set generation\n","  negative_sampling=np.concatenate((trans_matrix[trans_matrix[:,0]==customer_list[i]], create_notfind))\n","  negative_sampling_all = np.concatenate((negative_sampling_all,negative_sampling))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2kWU85XsHqdo"},"outputs":[],"source":["# convert to dataframe\n","train_set = pd.DataFrame(negative_sampling_all, columns = ['customer_id','article_id','interaction'])\n","train_set=train_set.astype(int)\n","customer0=train_set[train_set.customer_id==0]\n","customer0[customer0.interaction==0]\n","train_set"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4Af3bYTtPcpr"},"outputs":[],"source":["# Load model from h5 file\n","\n","my_model = keras.models.load_model('my_model/my_model_12_d')\n","my_model.load_weights(\"my_model/weights_12_d.h5\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iEnX5p8dysBI"},"outputs":[],"source":["#Model implementation with keras\n","\n","N = interactions.customer_id.unique() #Number of customers in the data\n","M = interactions.article_id.unique() #Number of articles in the data\n","n_users, n_item = len(N), len(M)\n","\n","n_latent_factors = 64 #Latent factors; K must be a power of two\n","\n","#articles input\n","item_input = keras.layers.Input(shape=[1],name='Item')\n","#articles embedding\n","item_embedding = keras.layers.Embedding(n_item + 1, n_latent_factors, name='Item-Embedding')(item_input)\n","#articles vector\n","item_vec = keras.layers.Flatten(name='FlattenItems')(item_embedding)\n","#customer input\n","user_input = keras.layers.Input(shape=[1],name='User')\n","#customer embedding\n","user_embedding = keras.layers.Embedding(n_users + 1, n_latent_factors,name='User-Embedding')(user_input)\n","#customer vector\n","user_vec = keras.layers.Flatten(name='FlattenUsers')(user_embedding)\n","#dot product\n","prod = keras.layers.dot([item_vec, user_vec], axes=1,name='DotProduct')\n","#add dropout to avoid overfitting\n","drop = keras.layers.Dropout(0.2, name='Dropout')(prod)\n","#add Sigmoid activation function to normalize outputs from 0 to 1\n","act = keras.layers.Dense(1, activation='sigmoid', name='Activation')(drop)\n","\n","#generate the model\n","model = keras.Model([user_input, item_input], drop)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"e9ykRIZ0y20Y"},"outputs":[],"source":["#compile the model\n","#Adam Optimizer uses GD; BCE as loss function \n","model.compile(optimizer='Adam', loss='binary_crossentropy')\n","#visualize model\n","tf.keras.utils.plot_model(model, to_file='model.png')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kIo0mbXfKIYE"},"outputs":[],"source":["start_time = time.time()\n","#train the model\n","history = model.fit([train_set.customer_id, train_set.article_id], train_set.interaction, epochs=12, verbose=0)\n","pd.Series(history.history['loss']).plot(logy=True)\n","plt.xlabel(\"Epoch\")\n","plt.ylabel(\"Training Error\")\n","\n","end_time = time.time()\n"," \n","print(\"The time of execution of above program (in minutes) is :\", (end_time - start_time)/60)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"V7PT4Z7lgpta"},"outputs":[],"source":["# Save keras model in h5 file\n","# model.save(\"modelname\")\n","# model.save_weights(\"weightsname.h5\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3_5jdL8aLsWE"},"outputs":[],"source":["#Example of one learnt article embedding with k latent factors\n","article_embedding_learnt = model.get_layer(name='Item-Embedding').get_weights()[0]\n","pd.DataFrame(article_embedding_learnt).describe()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kiJaEpNxRXa0"},"outputs":[],"source":["#Example of one learnt customer embedding with k latent factors\n","customer_embedding_learnt = model.get_layer(name='User-Embedding').get_weights()[0]\n","pd.DataFrame(customer_embedding_learnt).describe()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MN2epxKKgptd"},"outputs":[],"source":["#Evaluation of the recommender model: "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_ZtGXnP0gptd"},"outputs":[],"source":["#We create a function that outputs the best recommended articles to buy for a given user not taking into account articles that the customer has already bought. \n","def recommend(user_id, number_of_articles=50):\n","  #Ranking value of each article associated by the model \n","  articles = customer_embedding_learnt[user_id]@article_embedding_learnt.T \n","  #Obtain the first 50 articles with highest value\n","  mids = np.argpartition(articles, -number_of_articles)[-number_of_articles:]\n","  #Get the articles that are already bought by each customer\n","  art_buy=np.array(pre_train_set[pre_train_set['customer_id']==user_id].article_id)\n","  #Get the first 10 articles without considering the ones that are already bought. \n","  mids_not_buy = [i for i in mids if i not in art_buy][0:10]\n","  return mids_not_buy"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gDHn6bZUgptd"},"outputs":[],"source":["#Function that checks if the last bought article (test_set) belongs to the recommended list of top10 articles. \n","def heat_ratio_recommend(df):\n","  exito_recommend=[]\n","  for column in df['customer_id'].unique().tolist():\n","    #obtains the top10 articles of each customer\n","    candidates = recommend(column)\n","    #Checks if the article from the test_set is in the candidates list\n","    if df[df.customer_id == column].article_id.tolist() in candidates: \n","      exito_recommend.append(1)\n","    else:\n","      exito_recommend.append(0)\n","  return(exito_recommend) "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nr6KKo2hgpte"},"outputs":[],"source":["#HR from the recommendation based on top10 scores for each customer: mean of succes. \n","hr_recommend=np.mean(heat_ratio_recommend(test_set))\n","hr_recommend"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XWOTSFFPgpte"},"outputs":[],"source":["#Concatenate all recommended articles in one array. \n","recommend_model = []\n","for x in range(np.size(customer_list)):\n","   myArray = recommend(x)\n","   recommend_model += myArray"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KtmETXYygpte"},"outputs":[],"source":["#Calculate the coverage to see how many articles have been recommended by the model over the total. \n","cov_recommend=(np.size(np.unique(recommend_model))/np.size(np.unique(pre_train_set.article_id)))*100\n","cov_recommend"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Kui-CFlBgpte"},"outputs":[],"source":["#Evaluation of the random recommender: "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HwRx2YhWgptf"},"outputs":[],"source":["#We create a function that outputs 10 articles randomly not taking into account articles that the customer has already bought. \n"," #list of all articles\n","all_art = np.unique(train_set.article_id)\n","def random(user_id):\n","  #we select 50 randomly\n","  art_random =np.random.choice(all_art, 50)\n","  #check the articles already bought by the customer\n","  art_buy=np.array(pre_train_set[pre_train_set['customer_id']==user_id].article_id)\n","  #select 10 of the randoms without considering the ones in art_buy\n","  mids_random = [i for i in art_random if i not in art_buy][0:10]\n","  return mids_random"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"V_72f7gLgptf"},"outputs":[],"source":["#Join all randomly recommended articles for all customers \n","random_recommend = []\n","for x in range(np.size(customer_list)):\n","   myArray = random(x)\n","   random_recommend += myArray"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nyrbfeKpgptf"},"outputs":[],"source":["#Calculate the coverage to see how many articles have been recommended randomly over the total. \n","cov_random=(np.size(np.unique(random_recommend))/np.size(np.unique(pre_train_set.article_id)))*100\n","cov_random"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ww2Nnp6Vgptf"},"outputs":[],"source":["#Evaluate results: HEAT RATIO for random recommendation: \n","def heat_ratio_random(df):\n","  exito_random=[]\n","  for column in df['customer_id'].unique().tolist():\n","    #In this case candidates come from the articles recommended randomly by the random function. \n","    candidates = random(column)\n","    if df[df.customer_id == column].article_id.tolist() in candidates:\n","      exito_random.append(1)\n","    else:\n","      exito_random.append(0)\n","  return(exito_random) "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"b_4ChG_zgptg"},"outputs":[],"source":["#HR from the recommendation based on the 10 randomly choosen articles\n","hr_random=np.mean(heat_ratio_random(test_set))\n","hr_random"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"r8O6nFzXgptg"},"outputs":[],"source":["#Evaluation of the popularity recommender: we recommend to all customers the most popular articles. \n","interactions.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iO8jC0TFgptg"},"outputs":[],"source":["# Get top 10 most bought articles --> most populars\n","# we drop all interactions that appear several times on a given customer\n","pre_train_unique=[]\n","for i in range(np.size(customer_list)):\n","    articles_unique=pre_train_set[pre_train_set.customer_id==i].article_id.drop_duplicates().tolist()\n","    pre_train_unique += articles_unique\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CsGp4R_kPcqH"},"outputs":[],"source":["#conversion to dataframe\n","pre_train_unique=pd.DataFrame(pre_train_unique, columns = ['article_id'])\n","pre_train_unique"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pOIFTdZUPcqI"},"outputs":[],"source":["# Output -> 10 most bought articles\n","n = 10\n","mids_popular_unique=pre_train_unique['article_id'].value_counts()[:n].index.tolist()\n","mids_popular_unique"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SzLt-DwC-vMV"},"outputs":[],"source":["#Evaluate results: HEAT RATIO for common recommendations --> random or popular\n","def heat_ratio_common(df,mids):\n","  exito=[]\n","  #It checks the test_set with the 10 most popular articles (all equal for all customers)\n","  for column in df['customer_id'].unique().tolist():\n","    if all(item in mids for item in df[df.customer_id == column].article_id.tolist()):\n","      exito.append(1)\n","    else:\n","      exito.append(0)\n","  return(exito) "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NI-ivtnp_j_4"},"outputs":[],"source":["# #HR from the recommendation based on the 10 most popular recommended articles\n","hr_popularity=np.mean(heat_ratio_common(test_set,mids_popular_unique))\n","hr_popularity"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rllft9pmgpth"},"outputs":[],"source":["#Calculate the coverage to see how many articles have been recommended over the total considering the ten most popular articles.  \n","cov_popular=(np.size(mids_popular_unique)/np.size(np.unique(pre_train_set.article_id)))*100\n","cov_popular"]}],"metadata":{"colab":{"collapsed_sections":[],"name":"CAPSTONE_PROJECT_final.ipynb","provenance":[]},"interpreter":{"hash":"506d569f0b28ea103188d4d9f745b7b2b85d3e31dafde01bdf7899299ac0033b"},"kernelspec":{"display_name":"Python 3.8.8 ('base')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"}},"nbformat":4,"nbformat_minor":0}